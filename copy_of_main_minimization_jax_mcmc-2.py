# -*- coding: utf-8 -*-
"""Copy of @ main_minimization_jax_MCMC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fystmMgL431HQU2G8LvSZ3VXyaaVHo0E
"""

!pip install jaxopt --quiet

from jaxopt import ScipyMinimize as minimize

from jax.experimental import sparse as jsparse

import argparse
import os
import time

import numpy as np
from scipy.special import expit

import jax.numpy as jnp
import jax.random as random
import jax

RANDOM_SEED = 8927
np.random.seed(RANDOM_SEED)
import os
import numpy as np
from scipy import sparse, ndimage
import imageio as iio
from skimage.transform import resize

# from cr.sparse import lop
import matplotlib.pyplot as plt
# !pip install cr-sparse
!pip install gdown --quiet
import gdown
!pip install numpyro --quiet
!pip install git+https://github.com/austinpeel/herculens.git --quiet
from herculens.Util.jax_util import WaveletTransform
# from jax.scipy.optimize import minimize
import numpyro
from numpyro.diagnostics import summary
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS
RANDOM_SEED = 8927
np.random.seed(RANDOM_SEED)

# https://drive.google.com/file/d/15AUlevQVEM60z1OfL0B2FPJYsQ7pV2vu/view?usp=share_link
# https://drive.google.com/file/d/1pVTEMDj2aKdF-QctpuC8bcXmD9IoSowl/view?usp=share_link
def download_data():
  id1 = "15AUlevQVEM60z1OfL0B2FPJYsQ7pV2vu"
  id2 = "1pVTEMDj2aKdF-QctpuC8bcXmD9IoSowl"
  id3 = "1ZJXwjzxJtk-4PwPGogKKyVWqoPwYOcM0"
  url = "https://drive.google.com/uc?id="+id1
  output = 'L.txt'
  gdown.download(url, output, quiet=False)
  url = "https://drive.google.com/uc?id="+id2
  output = 'NGC.jpg'
  gdown.download(url, output, quiet=False)
  url = "https://drive.google.com/uc?id="+id3
  output = 'jamesweb.jpg'
  gdown.download(url, output, quiet=False)

def prepare_data():
    x = np.fromfile("L.txt",sep=" ").reshape(-1,3)
    Row=x[:, 0]
    int_row = Row.astype(int)
    Column=x[:,1]
    int_Column = Column.astype(int)
    Data=x[:,2]
    L = sparse.coo_matrix((Data,(int_row,int_Column)),(int_row.max()+1,40000))
    
    img = iio.imread("NGC.jpg")
    img  =  img.mean(axis=2)
    np_img = np.array(img)
    img = resize(img, (100, 100))
    length = np.prod(img.shape)
    s_true = img.reshape((length,1))
    L_T = L.transpose()
    D = L_T*s_true
    D_withNoise = D + np.random.randn(*D.shape)*10
    plt.imshow(D_withNoise.reshape((200,200)))
    plt.show()
    plt.imshow(s_true.reshape((n_pixels,n_pixels)))
    plt.show()

    return L_T, s_true, (D_withNoise),D

def prepare_data_n(n):
    x = np.fromfile("L.txt",sep=" ").reshape(-1,3)
    Row=x[:, 0]
    int_row = Row.astype(int)
    Column=x[:,1]
    int_Column = Column.astype(int)
    Data=x[:,2]
    # L=np.cos(0.0000001*np.arange(n*n*200*200)).reshape((200*200,n*n))
    L=np.identity(n*n)
    # L[6:100,6:100]=1
    # L=np.arange(256)
    img = iio.imread("NGC.jpg")
    img  =  img.mean(axis=2)
    np_img = np.array(img)
    img = resize(img, (n, n))
    length = np.prod(img.shape)
    s_true = img.reshape((length,1))
    L_T = L
    D = L_T@s_true
    D_withNoise = D + np.random.randn(*D.shape)*10
    plt.imshow(D_withNoise.reshape((n,n)))
    plt.show()
    plt.imshow(s_true.reshape((n,n)))
    plt.show()
    plt.imshow(L_T)
    plt.show()
    # L_T = L_T.todense()
    return L_T, s_true, (D_withNoise),D

download_data()

#parameters
global n_scales
global n_pixels
global L_T
global D_withNoise
global starlet_profile
global reg_func
n_scales=6
n_pixels=100

def l1(s):
  return jnp.mean(jnp.abs(s))

def l2(s):
  return jnp.mean(s**2)

# c = HalfCauchy(torch.tensor([1.0]))
# global factor
# def horse_shoe(s,C_p_inverse):
#   ta = c.sample().to(s.device)**2
#   hs = torch.sum(-1*torch.log(torch.log(1+(2*factor/(s**2)))))
#   # Beta= 
#   # print(hs.detach().cpu())
#   return hs

# def cauchy(s,C_p_inverse):
#   ta = c.sample().to(s.device)**2
#   cu = torch.sum(torch.log(ta+(s**2)))
#   return cu

def identity(s):
  return 0

# chi2 = (noisy_data - source_recon_jax)**2
# def model(x):
#   x = x.reshape((n_scales+1),n_pixels, n_pixels)
#   s_found = starlet_profile._reconstruct_2nd_gen(x)
#   D_estimated = L_T@s_found.reshape(n_pixels**2,1)
#   chi = (jnp.mean((D_estimated-D_withNoise)**2))
#   print(chi)
#   return chi + reg_func(jnp.ravel(x[1:,:,:]))

# L_T, s_true, D_withNoise,D = prepare_data_n(n_pixels)
# L_T = jsparse.coo.coo_fromdense(jnp.array(L_T))

L_T, s_true, D_withNoise,D = prepare_data()
L_T = jsparse.coo.coo_fromdense(jnp.array(L_T.todense()))

reg_func = l1
init_coeffs = jnp.squeeze(jnp.zeros(((n_scales+1)*n_pixels*n_pixels,1)))
x_ = init_coeffs
x = x_.reshape((n_scales+1),n_pixels, n_pixels)
reg_func(jnp.ravel(x[0:-1,:,:]))
starlet_profile = WaveletTransform(n_scales)
s_found = starlet_profile._reconstruct_2nd_gen(x)
D_estimated = L_T@s_found.reshape(n_pixels**2,1)
chi = (jnp.mean((D_estimated-D_withNoise)**2))
# chi_ = model(x_)

# solver = minimize(method='BFGS', fun=model)
# result = solver.run(init_params=init_coeffs)

#minimize(chi2)  ====> once optimized, this gives you the solution coeff_sol
# result = minimize((model),init_coeffs,method="BFGS",options={'maxiter':1000})

#reconstruction = starlet_profile_jax._reconstruct_2nd_gen(coeff_sol)
# params, state = result

# xx = params.reshape((n_scales+1),n_pixels, n_pixels)
# x_found = starlet_profile._reconstruct_2nd_gen(xx)

# #plt.imshow(reconstruction)

# plt.imshow((x_found))
# plt.title("Reconstruction with {}".format(reg_func.__name__))
# plt.savefig("Reconstruction with reg: {}".format((reg_func.__name__)))
# plt.show()

# for i in range(0,n_scales+1):
#   plt.imshow(xx[i,:,:])
#   plt.title("Coeff of {} scale".format(i))
#   plt.colorbar()
#   plt.savefig("Coeff of {} scale with reg: {}".format(i,(reg_func.__name__)))
#   plt.show()

# print(jnp.max(x_found))

#code for HS

# regression model with continuous-valued outputs/responses

def model_HS_likelihood(Y):
    alpha=1
    Tau = numpyro.sample("tau", dist.HalfCauchy(jnp.ones(alpha)))
    with numpyro.plate('wavelet', n_scales+1):
     with numpyro.plate('local_shrinkage', n_pixels*n_pixels):
       Lambda = numpyro.sample("lambdas", dist.HalfCauchy(scale=jnp.ones((1))))
       coeffs = numpyro.sample('coeffs', dist.Normal(loc=0, scale=Tau**2*Lambda**2))

    s_found = numpyro.deterministic('s_found',(starlet_profile._reconstruct_2nd_gen(coeffs.reshape((n_scales+1,n_pixels,n_pixels))).reshape(n_pixels*n_pixels,1)))
    mean_function = (L_T @ (s_found))
    sigma_obs = 0.1

    numpyro.sample("Y", dist.Normal(mean_function, sigma_obs), obs=Y)

# regression model with continuous-valued outputs/responses
def model_HS_likelihood_with_const_tau(Y):
   
    with numpyro.plate('wavelet', n_scales+1):
     with numpyro.plate('local_shrinkage', n_pixels*n_pixels):
       Lambda = numpyro.sample("lambdas", dist.HalfCauchy(scale=jnp.ones((1))))
       coeffs = numpyro.sample('coeffs', dist.Normal(loc=0, scale=Tau**2*Lambda**2))

    s_found = numpyro.deterministic('s_found',(starlet_profile._reconstruct_2nd_gen(coeffs.reshape((n_scales+1,n_pixels,n_pixels))).reshape(n_pixels*n_pixels,1)))

    mean_function = (L_T @ (s_found))
    sigma_obs = 0.1

    numpyro.sample("Y", dist.Normal(mean_function, sigma_obs), obs=Y)

# regression model with continuous-valued outputs/responses
def model_HS_withoutStarlet_likelihood(Y):
    Tau = numpyro.sample("tau", dist.HalfCauchy(jnp.ones(1)))
    with numpyro.plate('local_shrinkage', n_pixels*n_pixels):
      Lambda = numpyro.sample("lambdas", dist.HalfCauchy(scale=jnp.ones((1))))
      coeffs = numpyro.sample('coeffs', dist.Normal(loc=0, scale=Tau**2*Lambda**2))

    s_found = numpyro.deterministic('s_found',coeffs.reshape(n_pixels*n_pixels,1))
    mean_function = (L_T @ (s_found))
    sigma_obs = 0.1
    numpyro.sample("Y", dist.Normal(mean_function, sigma_obs), obs=Y)

# regression model with continuous-valued outputs/responses
def model_L2_likelihood(Y):


    Tau = numpyro.sample("tau", dist.HalfCauchy(jnp.ones(1)))
    with numpyro.plate('wavelet', n_scales+1):
     with numpyro.plate('local_shrinkage', n_pixels*n_pixels):
       coeffs = numpyro.sample('coeffs', dist.Normal(loc=0, scale=Tau))
      # coeffs = numpyro.deterministic("coeffs",  horseshoe_sigma * unscaled_coeffs)

    s_found = numpyro.deterministic('s_found',jax.nn.relu(starlet_profile._reconstruct_2nd_gen(coeffs.reshape((n_scales+1,n_pixels,n_pixels))).reshape(n_pixels*n_pixels,1)))
    # sigma_obs = numpyro.sample("sigma", dist.HalfNormal(128*jnp.ones(1)))
    mean_function = (L_T @ (s_found))
    # mean_function  = starlet_profile._reconstruct_2nd_gen(s_found)
    # note that in practice for a normal likelihood we would probably want to
    # integrate out the coefficients (as is done for example in sparse_regression.py).
    # however, this trick wouldn't be applicable to other likelihoods
    # (e.g. bernoulli, see below) so we don't make use of it here.
    # unscaled_betas = numpyro.sample("unscaled_betas", dist.Normal(0.0, jnp.ones(D_X)))
    # scaled_betas = numpyro.deterministic("betas", tau * lambdas * unscaled_betas)

    # compute mean function using linear coefficients
    # mean_function = jnp.dot(X, scaled_betas)

    # prec_obs = numpyro.sample("prec_obs", dist.Gamma(3.0, 1.0))
    # sigma_obs = 1.0 / jnp.sqrt(prec_obs)
    sigma_obs = 0.1
    # observe data
    numpyro.sample("Y", dist.Normal(mean_function, sigma_obs), obs=Y)

# regression model with continuous-valued outputs/responses
def model_L1_likelihood(Y):


    Tau = numpyro.sample("tau", dist.HalfCauchy(jnp.ones(1)))
    with numpyro.plate('wavelet', n_scales+1):
     with numpyro.plate('local_shrinkage', n_pixels*n_pixels):
       coeffs = numpyro.sample('coeffs', dist.Laplace(loc=0, scale=Tau))
      # coeffs = numpyro.deterministic("coeffs",  horseshoe_sigma * unscaled_coeffs)

    s_found = numpyro.deterministic('s_found',(starlet_profile._reconstruct_2nd_gen(coeffs.reshape((n_scales+1,n_pixels,n_pixels))).reshape(n_pixels*n_pixels,1)))
    # sigma_obs = numpyro.sample("sigma", dist.HalfNormal(128*jnp.ones(1)))
    mean_function = (L_T @ jax.nn.relu(s_found))
    # mean_function  = starlet_profile._reconstruct_2nd_gen(s_found)
    # note that in practice for a normal likelihood we would probably want to
    # integrate out the coefficients (as is done for example in sparse_regression.py).
    # however, this trick wouldn't be applicable to other likelihoods
    # (e.g. bernoulli, see below) so we don't make use of it here.
    # unscaled_betas = numpyro.sample("unscaled_betas", dist.Normal(0.0, jnp.ones(D_X)))
    # scaled_betas = numpyro.deterministic("betas", tau * lambdas * unscaled_betas)

    # compute mean function using linear coefficients
    # mean_function = jnp.dot(X, scaled_betas)

    # prec_obs = numpyro.sample("prec_obs", dist.Gamma(3.0, 1.0))
    # sigma_obs = 1.0 / jnp.sqrt(prec_obs)
    sigma_obs = 0.1
    # observe data
    numpyro.sample("Y", dist.Normal(mean_function, sigma_obs), obs=Y)

# helper function for MCMC inference
def run_inference(model, args, rng_key, Y):
    start = time.time()
    kernel = NUTS(model)
    mcmc = MCMC(
        kernel,
        num_warmup=args.num_warmup,
        num_samples=args.num_samples,
        num_chains=args.num_chains,
        progress_bar=False if "NUMPYRO_SPHINXBUILD" in os.environ else True,
    )

    mcmc.run(rng_key,Y)
    # mcmc.print_summary(exclude_deterministic=False)

    samples = mcmc.get_samples()
    summary_dict = summary(samples, group_by_chain=False)

    print("\nMCMC elapsed time:", time.time() - start)

    return summary_dict

#@markdown <h3> ⚙️ Model Configuration</h3>
model = "HS + starlet" #@param ["HS + constant tau", "HS + starlet", "HS", "L1 + starlet", "L2 + starlet"]
warmup = "1000" #@param ["100", "200", "500", "1000", "2000", "10", ""]
samples = "1000" #@param ["100", "200", "500", "1000", "2000", "10", ""]

from jax.experimental import sparse as jsparse
def main(args):
    # Y= D_withNoise
    # do inference
    rng_key, rng_key_predict = random.split(random.PRNGKey(0))
    summary_ = run_inference(args.model, args, rng_key, D_withNoise)
    return summary_


class Argument:
    def __init__(self, num_samples, num_warmup, num_chains, num_data, device,model):
        self.num_samples = num_samples
        self.num_warmup = num_warmup
        self.num_chains = num_chains
        self.device = device
        self.model=model

if model == "HS + constant tau":
  model = model_HS_likelihood_with_const_tau
elif model == "HS + starlet": 
  model = model_HS_likelihood
elif model == "HS": 
  model = model_HS_withoutStarlet_likelihood
elif model == "L1 + starlet":
  model = model_L1_likelihood
elif model == "L2 + starlet":
  model = model_L2_likelihood
args = Argument(int(warmup), int(samples), 1, 50, 'cpu',model)
summary_ = main(args)
dir = "bla/"



vmin, vmax = -50,300
plt.imshow(summary_['s_found']['mean'].reshape(n_pixels,n_pixels),vmin=vmin, vmax=vmax)
plt.colorbar()
plt.show()
# Distribution of s_found's pixels
plt.hist(summary_['s_found']['mean'])
plt.savefig("Distribution of s_found's pixels")
#plt.savefig(dir+ "Distribution of s_found's pixels")
plt.show()
# plt.hist(summary['unscaled_coeffs']['mean'])
# plt.show()
#Distribution of Coeffs
plt.hist(summary_['coeffs']['mean'].flatten())
plt.savefig("Distribution of Coeffs")
plt.show()
# lambda should only be large for the first 3 dimensions, which
# # correspond to relevant covariates (see get_data)
# print("Posterior median over lambdas (leading 5 dimensions):")
# print(summary["lambdas"]["median"][:5])
# print("Posterior mean over betas (leading 5 dimensions):")
# print(summary["betas"]["mean"][:5])

summary_['s_found']['mean'].min()

Name= "HS & Starlet "
#plt.imshow(reconstruction)

xx= summary_['coeffs']['mean'].reshape((n_scales+1,n_pixels,n_pixels))
for i in range(0,n_scales+1):
  plt.imshow(xx[i,:,:])
  plt.title("Coeff of {} scale".format(i))
  vmin, vmax = xx.min(), xx.max()
  plt.imshow(xx[i,:,:],vmin=vmin, vmax=vmax)
  plt.colorbar()
  plt.savefig("Coeff of {} scale with reg: {}".format(i,Name))
  plt.show()
x_found= summary_['s_found']['mean'].reshape(n_pixels,n_pixels)
plt.imshow((x_found),vmin=vmin, vmax=vmax)
plt.title("Reconstruction with {}".format(Name + " & MCMC"))
plt.savefig("Reconstruction with reg: {}".format(Name))

plt.show()

# lambdas samples
# xx= summary_['lambdas']['mean'].reshape((n_scales+1,n_pixels,n_pixels))
# for i in range(0,n_scales+1):
#   plt.imshow(xx[i,:,:])
#   plt.title("Coeff of {} scale".format(i))
#   vmin, vmax = xx.min(), xx.max()
#   plt.imshow(xx[i,:,:],vmin=vmin, vmax=vmax)
#   plt.colorbar()
#   plt.savefig("Coeff of {} scale with reg: {}".format(i,Name))
#   plt.show()

#@title
for Tau in [0.1,1,5]:
  summary_ = main(args)

  s_found = summary_['s_found']['mean']

  print("Mean squared error:", mse(s_found,s_true))
  print("ssim:", ssim(s_found.reshape(-1),s_true.reshape(-1)))

def mse(x,y):
    mse = np.mean((x-y)**2)
    return mse


import numpy as np

def ssim(a, b, k1=0.01, k2=0.03, L=255):
    """
    Compute the Structural Similarity Index (SSIM) between two grayscale images.

    Parameters:
    a (ndarray): Pixel values of the first image.
    b (ndarray): Pixel values of the second image (same size as a).
    k1 (float): SSIM parameter controlling the impact of the luminance difference on the score.
    k2 (float): SSIM parameter controlling the impact of the contrast difference on the score.
    L (float): Maximum pixel value (usually 255 for 8-bit images).

    Returns:
    float: The SSIM score between a and b, ranging from -1 (completely dissimilar) to 1 (identical).
    """
    c1 = (k1 * L) ** 2
    c2 = (k2 * L) ** 2
    mu_a = np.mean(a)
    mu_b = np.mean(b)
    var_a = np.var(a)
    var_b = np.var(b)
    cov_ab = np.cov(a, b)[0][1]
    num = (2 * mu_a * mu_b + c1) * (2 * cov_ab + c2)
    den = (mu_a ** 2 + mu_b ** 2 + c1) * (var_a + var_b + c2)
    return num / den

s_found = summary_['s_found']['mean']

print("Mean squared error:", mse(s_found,s_true))
print("ssim:", ssim(s_found.reshape(-1),s_true.reshape(-1)))

#STD 
Name= "HS & Starlet "
xx= summary_['coeffs']['std'].reshape((n_scales+1,n_pixels,n_pixels))
for i in range(0,n_scales+1):
  plt.imshow(xx[i,:,:])
  plt.title("Coeff of {} scale".format(i))
  vmin, vmax = xx.min(), xx.max()
  plt.imshow(xx[i,:,:],vmin=vmin, vmax=vmax)
  plt.colorbar()
  plt.savefig("Coeff of {} scale with reg: {}".format(i,Name))
  plt.show()
x_found= summary_['s_found']['std'].reshape(n_pixels,n_pixels)
plt.imshow((x_found),vmin=vmin, vmax=vmax)
plt.title("Reconstruction with {}".format(Name + " & MCMC"))
plt.savefig("Reconstruction with reg: {}".format(Name))
plt.show()

def residual(x,y):
  return (x-y)/x

plt.imshow(residual(s_found,s_true).reshape(100,100))
plt.title("Residual")
plt.savefig("residual")

for Tau in [0.1,1,5]:
  args.model= model_HS_likelihood_with_const_tau
  summary_ = main(args)
  
  s_found = summary_['s_found']['mean']

  print("Mean squared error:", mse(s_found,s_true))
  print("ssim:", ssim(s_found.reshape(-1),s_true.reshape(-1)))